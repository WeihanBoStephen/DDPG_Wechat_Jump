{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the testing code for the model combines the DDPG and NALU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1251\n",
      "(1251, True)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf     \n",
    "import numpy as np          \n",
    "import os\n",
    "from game_environment import Data_Env\n",
    "import pymysql\n",
    "import opencv_jump as jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_test_data(step,score):\n",
    "    step = str(step)\n",
    "    score = str(score)\n",
    "    db = pymysql.connect(\"localhost\", \"root\", \"root\", \"test\")\n",
    "    cursor = db.cursor()\n",
    "    insert = \"INSERT INTO test(step,score) VALUES('%s','%s')\"%(step,score)\n",
    "    result = cursor.execute(insert)\n",
    "    db.close()\n",
    "    print(insert)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(env, start, end, noise_sigma, init_memory, model_dir, experiment_dir,\n",
    "                         actor, critic, memory,\n",
    "                         actor_lr, critic_lr, batch_size,\n",
    "                         gamma, tau=0.01):\n",
    "    \n",
    "    #build agent: action_range=(-1., 1.),reward_scale=1.\n",
    "    agent = DDPG(actor, critic, memory, env.observation_shape, env.action_shape,\n",
    "                 actor_lr=actor_lr, critic_lr=critic_lr, batch_size=batch_size,\n",
    "                 gamma=gamma, tau=tau)\n",
    "    saver = tf.train.Saver(max_to_keep=20)\n",
    "    #------add save dir--------\n",
    "    checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"model\")\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    #summary dir------------------\n",
    "    summary_dir = os.path.join(experiment_dir, \"summaries\")\n",
    "    if not os.path.exists(summary_dir):\n",
    "        os.makedirs(summary_dir)\n",
    "    summary_writer = tf.summary.FileWriter(summary_dir)\n",
    "    summary = tf.Summary()\n",
    "    episode_summary = tf.Summary()\n",
    "    with tf.Session() as sess:\n",
    "        latest_checkpoint = model_dir\n",
    "        if latest_checkpoint:\n",
    "            print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
    "            saver.restore(sess, latest_checkpoint)\n",
    "            agent.sess = sess\n",
    "        else:\n",
    "            print('Building new model...')\n",
    "            agent.initialize(sess)\n",
    "        print('Generating ',init_memory,' memory... Please reset game!')\n",
    "        nalu_model = nalu.load_model(\"./nalu_experiments/model_weights.h5\")\n",
    "        for i in range(init_memory):\n",
    "            wechat_score = 0\n",
    "            jump_step = 0\n",
    "            obs0 = env.reset()\n",
    "            while 1:\n",
    "                #get action\n",
    "                feed_dict = {agent.obs0: [obs0]}\n",
    "                dens_2 = sess.run(actor.dens_2,feed_dict=feed_dict)\n",
    "                nb_actions = sess.run(actor.nb_actions,feed_dict=feed_dict)\n",
    "                print(nb_actions.shape)\n",
    "                break\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from model import Actor, Critic\n",
    "from memory import Memory\n",
    "from ddpg import DDPG\n",
    "import cv2\n",
    "from game_environment import Jump_Env as wechat_env\n",
    "import nalu_model as nalu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_lr = 1e-4\n",
    "critic_lr = 1e-3\n",
    "tau = 0.01\n",
    "nb_actions = 1\n",
    "batch_size = 128\n",
    "limit=int(5000)\n",
    "noise_sigma = 0.1\n",
    "gamma = 0\n",
    "init_memory = 100\n",
    "# episodes = 10000\n",
    "model_dir = \"./experiments_new/checkpoints/model6300\"#None\n",
    "start = 0\n",
    "end = 2000\n",
    "experiment_dir = os.path.abspath(\"./experiments_new/\")\n",
    "number_templet = [cv2.imread('templet/{}.jpg'.format(i)) for i in range(10)]\n",
    "restart_templet = cv2.imread('templet/again.jpg')\n",
    "env = wechat_env(number_templet=number_templet, restart_templet=restart_templet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = Actor(nb_actions, layer_norm=True)\n",
    "critic = Critic(layer_norm=True)\n",
    "memory = Memory(limit, action_shape=env.action_shape, observation_shape=env.observation_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor\n",
      "Critic\n",
      "Critic\n",
      "Actor\n",
      "Critic\n",
      "setting up target updates ...\n",
      "len 16 = 16\n",
      "{ target_actor/Conv/weights:0 } <- { actor/Conv/weights:0 }\n",
      "{ target_actor/Conv/biases:0 } <- { actor/Conv/biases:0 }\n",
      "{ target_actor/Conv_1/weights:0 } <- { actor/Conv_1/weights:0 }\n",
      "{ target_actor/Conv_1/biases:0 } <- { actor/Conv_1/biases:0 }\n",
      "{ target_actor/Conv_2/weights:0 } <- { actor/Conv_2/weights:0 }\n",
      "{ target_actor/Conv_2/biases:0 } <- { actor/Conv_2/biases:0 }\n",
      "{ target_actor/dense/kernel:0 } <- { actor/dense/kernel:0 }\n",
      "{ target_actor/dense/bias:0 } <- { actor/dense/bias:0 }\n",
      "{ target_actor/LayerNorm/beta:0 } <- { actor/LayerNorm/beta:0 }\n",
      "{ target_actor/LayerNorm/gamma:0 } <- { actor/LayerNorm/gamma:0 }\n",
      "{ target_actor/dense_1/kernel:0 } <- { actor/dense_1/kernel:0 }\n",
      "{ target_actor/dense_1/bias:0 } <- { actor/dense_1/bias:0 }\n",
      "{ target_actor/LayerNorm_1/beta:0 } <- { actor/LayerNorm_1/beta:0 }\n",
      "{ target_actor/LayerNorm_1/gamma:0 } <- { actor/LayerNorm_1/gamma:0 }\n",
      "{ target_actor/dense_2/kernel:0 } <- { actor/dense_2/kernel:0 }\n",
      "{ target_actor/dense_2/bias:0 } <- { actor/dense_2/bias:0 }\n",
      "setting up target updates ...\n",
      "len 16 = 16\n",
      "{ target_critic/Conv/weights:0 } <- { critic/Conv/weights:0 }\n",
      "{ target_critic/Conv/biases:0 } <- { critic/Conv/biases:0 }\n",
      "{ target_critic/Conv_1/weights:0 } <- { critic/Conv_1/weights:0 }\n",
      "{ target_critic/Conv_1/biases:0 } <- { critic/Conv_1/biases:0 }\n",
      "{ target_critic/Conv_2/weights:0 } <- { critic/Conv_2/weights:0 }\n",
      "{ target_critic/Conv_2/biases:0 } <- { critic/Conv_2/biases:0 }\n",
      "{ target_critic/dense/kernel:0 } <- { critic/dense/kernel:0 }\n",
      "{ target_critic/dense/bias:0 } <- { critic/dense/bias:0 }\n",
      "{ target_critic/LayerNorm/beta:0 } <- { critic/LayerNorm/beta:0 }\n",
      "{ target_critic/LayerNorm/gamma:0 } <- { critic/LayerNorm/gamma:0 }\n",
      "{ target_critic/dense_1/kernel:0 } <- { critic/dense_1/kernel:0 }\n",
      "{ target_critic/dense_1/bias:0 } <- { critic/dense_1/bias:0 }\n",
      "{ target_critic/LayerNorm_1/beta:0 } <- { critic/LayerNorm_1/beta:0 }\n",
      "{ target_critic/LayerNorm_1/gamma:0 } <- { critic/LayerNorm_1/gamma:0 }\n",
      "{ target_critic/dense_2/kernel:0 } <- { critic/dense_2/kernel:0 }\n",
      "{ target_critic/dense_2/bias:0 } <- { critic/dense_2/bias:0 }\n",
      "setting up actor optimizer\n",
      "WARNING:tensorflow:From C:\\Users\\Stephen_Bo\\Desktop\\FYP\\Final_Code\\ddpg.py:13: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Loading model checkpoint ./experiments_new/checkpoints/model6300...\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from ./experiments_new/checkpoints/model6300\n",
      "Generating  100  memory... Please reset game!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Stephen_Bo\\Anaconda3\\lib\\site-packages\\skimage\\transform\\_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Fetch argument 1 has invalid type <class 'int'>, must be a string or Tensor. (Can not convert a int into a Tensor or Operation.)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[0;32m    281\u001b[0m         self._unique_fetches.append(ops.get_default_graph().as_graph_element(\n\u001b[1;32m--> 282\u001b[1;33m             fetch, allow_tensor=True, allow_operation=True))\n\u001b[0m\u001b[0;32m    283\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   3458\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3459\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   3547\u001b[0m       raise TypeError(\"Can not convert a %s into a %s.\" % (type(obj).__name__,\n\u001b[1;32m-> 3548\u001b[1;33m                                                            types_str))\n\u001b[0m\u001b[0;32m   3549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Can not convert a int into a Tensor or Operation.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-025bdd6ad648>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m test(env=env, start=start, end=end, noise_sigma=noise_sigma, init_memory=init_memory, model_dir=model_dir, experiment_dir=experiment_dir, actor=actor, critic=critic, memory=memory, \n\u001b[1;32m----> 2\u001b[1;33m               actor_lr=actor_lr, critic_lr=critic_lr, batch_size=batch_size, gamma=gamma, tau=tau)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-e0fe2ccf43c6>\u001b[0m in \u001b[0;36mtest\u001b[1;34m(env, start, end, noise_sigma, init_memory, model_dir, experiment_dir, actor, critic, memory, actor_lr, critic_lr, batch_size, gamma, tau)\u001b[0m\n\u001b[0;32m     40\u001b[0m                 \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobs0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mobs0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m                 \u001b[0mdens_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdens_2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m                 \u001b[0mnb_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[1;32m-> 1122\u001b[1;33m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[0;32m   1123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[0;32m    425\u001b[0m     \"\"\"\n\u001b[0;32m    426\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[1;34m(fetch)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m           \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetch_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0m_ElementFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m     \u001b[1;31m# Did not find anything.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     raise TypeError('Fetch argument %r has invalid type %r' % (fetch,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[0;32m    284\u001b[0m         raise TypeError('Fetch argument %r has invalid type %r, '\n\u001b[0;32m    285\u001b[0m                         \u001b[1;34m'must be a string or Tensor. (%s)'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m                         (fetch, type(fetch), str(e)))\n\u001b[0m\u001b[0;32m    287\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m         raise ValueError('Fetch argument %r cannot be interpreted as a '\n",
      "\u001b[1;31mTypeError\u001b[0m: Fetch argument 1 has invalid type <class 'int'>, must be a string or Tensor. (Can not convert a int into a Tensor or Operation.)"
     ]
    }
   ],
   "source": [
    "test(env=env, start=start, end=end, noise_sigma=noise_sigma, init_memory=init_memory, model_dir=model_dir, experiment_dir=experiment_dir, actor=actor, critic=critic, memory=memory, \n",
    "              actor_lr=actor_lr, critic_lr=critic_lr, batch_size=batch_size, gamma=gamma, tau=tau)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
