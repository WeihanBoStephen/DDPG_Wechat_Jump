{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Neural Accumulator (NAC) for addition/subtraction -> Useful to learn the addition/subtraction operation\n",
    "\n",
    "def nac_simple_single_layer(x_in, out_units):\n",
    "    '''\n",
    "    Define a Neural Accumulator (NAC) for addition/subtraction -> Useful to learn the addition/subtraction operation\n",
    "    Attributes:\n",
    "        x_in -> Input vector\n",
    "        out_units -> number of output neurons\n",
    "    Return:\n",
    "        Output tensor of mentioned shsape and associated weights\n",
    "    '''\n",
    "\n",
    "    in_features = x_in.shape[1]\n",
    "\n",
    "    # define W_hat and M_hat\n",
    "\n",
    "    W_hat = tf.get_variable(name = \"W_hat\", initializer=tf.initializers.random_uniform(minval=-2, maxval=2),shape=[in_features, out_units],  trainable=True)\n",
    "    M_hat = tf.get_variable(name = \"M_hat\", initializer=tf.initializers.random_uniform(minval=-2, maxval=2), shape=[in_features, out_units], trainable=True)\n",
    "\n",
    "    # Get W\n",
    "\n",
    "    W = tf.nn.tanh(W_hat) * tf.nn.sigmoid(M_hat)\n",
    "\n",
    "    y_out = tf.matmul(x_in,W)\n",
    "\n",
    "    return y_out,W\n",
    "\n",
    "# define a complex nac in log space -> for more complex arithmetic functions such as\n",
    "# multiplication, division and power\n",
    "\n",
    "def nac_complex_single_layer(x_in, out_units, epsilon = 0.000001):\n",
    "\n",
    "    '''\n",
    "    :param x_in: input feature vector\n",
    "    :param out_units: number of output units of the cell\n",
    "    :param epsilon: small value to avoid log(0) in the output result\n",
    "    :return: associated weight matrix and output tensor\n",
    "    '''\n",
    "\n",
    "    in_shape = x_in.shape[1]\n",
    "\n",
    "    W_hat = tf.get_variable(shape=[in_shape, out_units],\n",
    "                            initializer= tf.initializers.random_uniform(minval=-2, maxval=2),\n",
    "                            trainable=True, name=\"W_hat2\")\n",
    "\n",
    "    M_hat = tf.get_variable(shape=[in_shape, out_units],\n",
    "                            initializer=tf.initializers.random_uniform(minval=-2, maxval=2),\n",
    "                            trainable=True, name=\"M_hat2\")\n",
    "\n",
    "    W = tf.nn.tanh(W_hat) * tf.nn.sigmoid(M_hat)\n",
    "\n",
    "    # Express Input feature in log space to learn complex functions\n",
    "    x_modified = tf.log(tf.abs(x_in) + epsilon)\n",
    "\n",
    "    m = tf.exp( tf.matmul(x_modified, W) )\n",
    "\n",
    "    return m, W\n",
    "\n",
    "# Define a NALU having combination of NAC1 and NAC2\n",
    "\n",
    "def nalu(x_in, out_units, epsilon=0.000001, get_weights=False):\n",
    "    '''\n",
    "    :param x_in: input feature vector\n",
    "    :param out_units: number of output units of the cell\n",
    "    :param epsilon: small value to avoid log(0) in the output result\n",
    "    :param get_weights: True if want to get the weights of the model\n",
    "                        in return\n",
    "    :return: output tensor\n",
    "    :return: Gate weight matrix\n",
    "    :return: NAC1 (simple NAC) weight matrix\n",
    "    :return: NAC2 (complex NAC) weight matrix\n",
    "    '''\n",
    "\n",
    "    in_shape = x_in.shape[1]\n",
    "\n",
    "    # Get output of NAC1\n",
    "    a, W_simple = nac_simple_single_layer(x_in, out_units)\n",
    "\n",
    "    # Get output of NAC2\n",
    "    m, W_complex = nac_complex_single_layer(x_in, out_units, epsilon= epsilon)\n",
    "\n",
    "    # Gate signal layer\n",
    "    G = tf.get_variable(initializer=tf.random_normal_initializer(stddev=1.0),\n",
    "                        shape=[in_shape, out_units], name=\"Gate_weights\")\n",
    "\n",
    "    g = tf.nn.sigmoid( tf.matmul(x_in, G) )\n",
    "\n",
    "    y_out = g * a + (1 - g) * m\n",
    "\n",
    "    if(get_weights):\n",
    "        return y_out, G, W_simple, W_complex\n",
    "    else:\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Network by learning the adition\n",
    "\n",
    "# Generate a series of input number X1,X2 and X3 for training\n",
    "x1 =  np.arange(1000,11000, step=5, dtype= np.float32)\n",
    "x2 =  np.arange(500, 6500 , step=3, dtype= np.float32)\n",
    "x3 = np.arange(0, 2000, step = 1, dtype= np.float32)\n",
    "\n",
    "\n",
    "# Make any function of x1,x2 and x3 to try the network on\n",
    "y_train = (x1/4) + (x2/2) + x3**2\n",
    "#y_train = x1 + x2 + x3\n",
    "\n",
    "x_train = np.column_stack( (x1,x2,x3) )\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "# Generate a series of input number X1,X2 and X3 for testing\n",
    "x1 =  np.random.randint(0,1000, size= 200).astype(np.float32)\n",
    "x2 = np.random.randint(1, 500, size=200).astype(np.float32)\n",
    "x3 = np.random.randint(50, 150 , size=200).astype(np.float32)\n",
    "\n",
    "x_test = np.column_stack((x1,x2,x3))\n",
    "\n",
    "y_test = (x1/4) + (x2/2) + x3**2\n",
    "\n",
    "#y_test = x1 + x2 + x3\n",
    "\n",
    "print()\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "\n",
    "# Define the placeholder to feed the value at run time\n",
    "X = tf.placeholder(dtype=tf.float32, shape =[None , 3])    # Number of samples x Number of features (number of inputs to be added)\n",
    "Y = tf.placeholder(dtype=tf.float32, shape=[None,])\n",
    "\n",
    "# define the network\n",
    "# Here the network contains only one NAC cell (for testing)\n",
    "y_pred  = nalu(X, out_units=1)\n",
    "y_pred = tf.squeeze(y_pred)             # Remove extra dimensions if any\n",
    "\n",
    "# Mean Square Error (MSE)\n",
    "loss = tf.reduce_mean( (y_pred - Y) **2)\n",
    "#loss= tf.losses.mean_squared_error(labels=y_train, predictions=y_pred)\n",
    "\n",
    "\n",
    "\n",
    "# training parameters\n",
    "alpha = 0.005 # learning rate\n",
    "epochs = 30000\n",
    "\n",
    "\n",
    "optimize = tf.train.AdamOptimizer(learning_rate=alpha).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # pre training evaluate\n",
    "    print(\"Pre training MSE: \", sess.run (loss, feed_dict={X: x_test, Y:y_test}))\n",
    "    print()\n",
    "    cost_history = []\n",
    "\n",
    "    for i in range(epochs):\n",
    "        _, cost = sess.run([optimize, loss], feed_dict={X: x_train, Y: y_train})\n",
    "        print(\"epoch: {}, MSE: {}\".format(i, cost))\n",
    "        cost_history.append(cost)\n",
    "\n",
    "    # plot the MSE over each iteration\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(epochs),np.log(cost_history))  # Plot MSE on log scale\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.show()\n",
    "\n",
    "    print()\n",
    "    #print(W.eval())\n",
    "    #print()\n",
    "    # post training loss\n",
    "    print(\"Post training MSE: \", sess.run(loss, feed_dict={X: x_test, Y: y_test}))\n",
    "\n",
    "    print(\"Actual sum: \", y_test[0:10])\n",
    "    print()\n",
    "    y_hat = sess.run(y_pred, feed_dict={X: x_test, Y: y_test})\n",
    "    print(\"Predicted sum: \", y_hat[0:10] )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
